{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq\n",
    "    generally seq2seq algorithm is used, when input sequences and output sequences have different lengths (e.g. machine translation) and the entire input sequence is required in order to start predicting the target. \n",
    "Here we are predicting character to character representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*For more INFO, check here: https://blog.keras.io/a-ten-minute-introduction-to-sequence-to-sequence-learning-in-keras.html\n",
    "\n",
    "*For language translation datasets, click here: http://www.manythings.org/anki/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense,LSTM,Input\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size =64\n",
    "epochs = 100\n",
    "latent_dim = 256  #output dimensionality of the encoding space.\n",
    "num_samples = 10000  #number of samples to train on.\n",
    "data_path = \"datasets/Machine Language Translation datasets/English - French language dataset.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Vectorize the data\n",
    "input_texts = []  \n",
    "target_texts = []  \n",
    "input_characters = set()  #input_text\n",
    "target_characters = set()  #output text\n",
    "with open(data_path, 'r', encoding = 'utf-8') as f:\n",
    "    lines = f.read().split('\\n')\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split('\\t')\n",
    "    \n",
    "    #we use 'tab' as start sequence character for the targets, and '\\n' as end sequence character\n",
    "    target_text = '\\t' + target_text + '\\n'\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.\\tVa !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #2877272 (CM) & #1158250 (Wittydev)',\n",
       " 'Hi.\\tSalut !\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #509819 (Aiji)',\n",
       " 'Hi.\\tSalut.\\tCC-BY 2.0 (France) Attribution: tatoeba.org #538123 (CM) & #4320462 (gillux)']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lines[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\tVa !\\n',\n",
       " '\\tSalut !\\n',\n",
       " '\\tSalut.\\n',\n",
       " '\\tCours\\u202f!\\n',\n",
       " '\\tCourez\\u202f!\\n']"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Go.', 'Hi.', 'Hi.', 'Run!', 'Run!']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_len = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_len = max([len(txt) for txt in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A boat capsized.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#getting max_encoder_seq_len's text\n",
    "text = np.argmax([len(txt) for txt in input_texts])\n",
    "input_texts[text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "no. of samples for training:  10000\n",
      "no. of unique input tokens(Eng): 71\n",
      "no. of unique output tokens(French): 92\n",
      "Max sequence of input length(Eng): 16\n",
      "Max sequence of output length(Fre): 59\n"
     ]
    }
   ],
   "source": [
    "print('no. of samples for training: ', len(input_texts))\n",
    "print('no. of unique input tokens(Eng):', num_encoder_tokens)\n",
    "print('no. of unique output tokens(French):', num_decoder_tokens)\n",
    "print('Max sequence of input length(Eng):', max_encoder_seq_len)\n",
    "print('Max sequence of output length(Fre):', max_decoder_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char,i) for i,char in enumerate(input_characters)])\n",
    "\n",
    "target_token_index = dict([(char,i) for i,char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({' ': 0,\n",
       "  '!': 1,\n",
       "  '$': 2,\n",
       "  '%': 3,\n",
       "  '&': 4,\n",
       "  \"'\": 5,\n",
       "  ',': 6,\n",
       "  '-': 7,\n",
       "  '.': 8,\n",
       "  '0': 9,\n",
       "  '1': 10,\n",
       "  '2': 11,\n",
       "  '3': 12,\n",
       "  '5': 13,\n",
       "  '6': 14,\n",
       "  '7': 15,\n",
       "  '8': 16,\n",
       "  '9': 17,\n",
       "  ':': 18,\n",
       "  '?': 19,\n",
       "  'A': 20,\n",
       "  'B': 21,\n",
       "  'C': 22,\n",
       "  'D': 23,\n",
       "  'E': 24,\n",
       "  'F': 25,\n",
       "  'G': 26,\n",
       "  'H': 27,\n",
       "  'I': 28,\n",
       "  'J': 29,\n",
       "  'K': 30,\n",
       "  'L': 31,\n",
       "  'M': 32,\n",
       "  'N': 33,\n",
       "  'O': 34,\n",
       "  'P': 35,\n",
       "  'Q': 36,\n",
       "  'R': 37,\n",
       "  'S': 38,\n",
       "  'T': 39,\n",
       "  'U': 40,\n",
       "  'V': 41,\n",
       "  'W': 42,\n",
       "  'Y': 43,\n",
       "  'a': 44,\n",
       "  'b': 45,\n",
       "  'c': 46,\n",
       "  'd': 47,\n",
       "  'e': 48,\n",
       "  'f': 49,\n",
       "  'g': 50,\n",
       "  'h': 51,\n",
       "  'i': 52,\n",
       "  'j': 53,\n",
       "  'k': 54,\n",
       "  'l': 55,\n",
       "  'm': 56,\n",
       "  'n': 57,\n",
       "  'o': 58,\n",
       "  'p': 59,\n",
       "  'q': 60,\n",
       "  'r': 61,\n",
       "  's': 62,\n",
       "  't': 63,\n",
       "  'u': 64,\n",
       "  'v': 65,\n",
       "  'w': 66,\n",
       "  'x': 67,\n",
       "  'y': 68,\n",
       "  'z': 69,\n",
       "  'é': 70},\n",
       " {'\\t': 0,\n",
       "  '\\n': 1,\n",
       "  ' ': 2,\n",
       "  '!': 3,\n",
       "  '%': 4,\n",
       "  '&': 5,\n",
       "  \"'\": 6,\n",
       "  '(': 7,\n",
       "  ')': 8,\n",
       "  ',': 9,\n",
       "  '-': 10,\n",
       "  '.': 11,\n",
       "  '0': 12,\n",
       "  '1': 13,\n",
       "  '2': 14,\n",
       "  '3': 15,\n",
       "  '5': 16,\n",
       "  '8': 17,\n",
       "  '9': 18,\n",
       "  ':': 19,\n",
       "  '?': 20,\n",
       "  'A': 21,\n",
       "  'B': 22,\n",
       "  'C': 23,\n",
       "  'D': 24,\n",
       "  'E': 25,\n",
       "  'F': 26,\n",
       "  'G': 27,\n",
       "  'H': 28,\n",
       "  'I': 29,\n",
       "  'J': 30,\n",
       "  'K': 31,\n",
       "  'L': 32,\n",
       "  'M': 33,\n",
       "  'N': 34,\n",
       "  'O': 35,\n",
       "  'P': 36,\n",
       "  'Q': 37,\n",
       "  'R': 38,\n",
       "  'S': 39,\n",
       "  'T': 40,\n",
       "  'U': 41,\n",
       "  'V': 42,\n",
       "  'Y': 43,\n",
       "  'a': 44,\n",
       "  'b': 45,\n",
       "  'c': 46,\n",
       "  'd': 47,\n",
       "  'e': 48,\n",
       "  'f': 49,\n",
       "  'g': 50,\n",
       "  'h': 51,\n",
       "  'i': 52,\n",
       "  'j': 53,\n",
       "  'k': 54,\n",
       "  'l': 55,\n",
       "  'm': 56,\n",
       "  'n': 57,\n",
       "  'o': 58,\n",
       "  'p': 59,\n",
       "  'q': 60,\n",
       "  'r': 61,\n",
       "  's': 62,\n",
       "  't': 63,\n",
       "  'u': 64,\n",
       "  'v': 65,\n",
       "  'x': 66,\n",
       "  'y': 67,\n",
       "  'z': 68,\n",
       "  '\\xa0': 69,\n",
       "  '«': 70,\n",
       "  '»': 71,\n",
       "  'À': 72,\n",
       "  'Ç': 73,\n",
       "  'É': 74,\n",
       "  'Ê': 75,\n",
       "  'à': 76,\n",
       "  'â': 77,\n",
       "  'ç': 78,\n",
       "  'è': 79,\n",
       "  'é': 80,\n",
       "  'ê': 81,\n",
       "  'ë': 82,\n",
       "  'î': 83,\n",
       "  'ï': 84,\n",
       "  'ô': 85,\n",
       "  'ù': 86,\n",
       "  'û': 87,\n",
       "  'œ': 88,\n",
       "  '\\u2009': 89,\n",
       "  '’': 90,\n",
       "  '\\u202f': 91})"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_token_index, target_token_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros((len(input_texts), max_encoder_seq_len, num_encoder_tokens), dtype = 'float32')\n",
    "\n",
    "decoder_input_data =  np.zeros((len(input_texts), max_decoder_seq_len, num_decoder_tokens), dtype = 'float32')\n",
    "\n",
    "decoder_target_data = np.zeros((len(input_texts), max_decoder_seq_len, num_decoder_tokens), dtype = 'float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 16, 71), (10000, 59, 92), (10000, 59, 92))"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### applying one-hot representiation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.\n",
    "    encoder_input_data[i, t+1:, input_token_index[' ']] = 1.  #replacing all with 1. with space character indexes..\n",
    "    \n",
    "    for t, char in enumerate(target_text):\n",
    "        #decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i,t, target_token_index[char]] = 1.\n",
    "        \n",
    "        if t > 0:\n",
    "            #decoder_target_data will be ahead by one timestep and will not include the start character('\\t')\n",
    "            decoder_target_data[i,t-1, target_token_index[char]] = 1.\n",
    "            \n",
    "    decoder_input_data[i, t+1:, target_token_index[' ']] = 1.\n",
    "    decoder_target_data[i, t:, target_token_index[' ']] = 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN ENCODER \n",
    "#Define an input sequence and preprocess it.\n",
    "encoder_inputs = Input(shape =(None, num_encoder_tokens))\n",
    "encoder = LSTM(latent_dim, return_state = True) \n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)  #return_state = True -- gives the output along with state_h and state_c\n",
    "#we discard 'encoder_outputs' and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"lstm/Identity_1:0\", shape=(None, 256), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "print(state_h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#RNN DECODER\n",
    "#setup the decoder, using 'encoder_states' as initial state\n",
    "decoder_inputs = Input(shape = (None, num_decoder_tokens))\n",
    "#we set up out decoder to return full output sequences and to return inernal states as well.\n",
    "#we don't use the return states in the training model, but we will use them in prediction.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state= encoder_states)\n",
    "decoder_dense = Dense(num_decoder_tokens, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NOTE:\n",
    "    encoder LSTM doesn't pass directly it's outputs as inputs to the decoder LSTM instead the decoder uses the final hidden and cell states as the initial state for decoder.\n",
    "    \n",
    "    Also note that Decoder LSTM pass the whole sequence of hidden states to the Dense (output layer) for output, instead of the final  hidden and cell state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define the model that will turn\n",
    "#'encoder_input_data' and 'decoder_input_data' into 'decoder_target_data'\n",
    "model = Model(inputs = [encoder_inputs, decoder_inputs], outputs = decoder_outputs)\n",
    "\n",
    "model.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graphical representation of encoder - decoder model :\n",
    "https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Graph-of-Encoder-Decoder-Model-For-Training.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(None, None, 71)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_2 (InputLayer)            [(None, None, 92)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm (LSTM)                     [(None, 256), (None, 335872      input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  357376      input_2[0][0]                    \n",
      "                                                                 lstm[0][1]                       \n",
      "                                                                 lstm[0][2]                       \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 92)     23644       lstm_1[0][0]                     \n",
      "==================================================================================================\n",
      "Total params: 716,892\n",
      "Trainable params: 716,892\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Begins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 1.1781 - accuracy: 0.7235 - val_loss: 1.1070 - val_accuracy: 0.6977\n",
      "Epoch 2/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 0.8515 - accuracy: 0.7707 - val_loss: 0.8366 - val_accuracy: 0.7717\n",
      "Epoch 3/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 0.6770 - accuracy: 0.8081 - val_loss: 0.7280 - val_accuracy: 0.7920\n",
      "Epoch 4/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.5929 - accuracy: 0.8273 - val_loss: 0.6481 - val_accuracy: 0.8113\n",
      "Epoch 5/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.5447 - accuracy: 0.8410 - val_loss: 0.6080 - val_accuracy: 0.8208\n",
      "Epoch 6/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.5097 - accuracy: 0.8503 - val_loss: 0.5784 - val_accuracy: 0.8288\n",
      "Epoch 7/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.4814 - accuracy: 0.8582 - val_loss: 0.5599 - val_accuracy: 0.8350\n",
      "Epoch 8/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.4571 - accuracy: 0.8647 - val_loss: 0.5432 - val_accuracy: 0.8390\n",
      "Epoch 9/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.4355 - accuracy: 0.8705 - val_loss: 0.5182 - val_accuracy: 0.8463\n",
      "Epoch 10/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.4160 - accuracy: 0.8758 - val_loss: 0.5032 - val_accuracy: 0.8509\n",
      "Epoch 11/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.3983 - accuracy: 0.8806 - val_loss: 0.4933 - val_accuracy: 0.8536\n",
      "Epoch 12/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.3827 - accuracy: 0.8851 - val_loss: 0.4817 - val_accuracy: 0.8578\n",
      "Epoch 13/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.3680 - accuracy: 0.8897 - val_loss: 0.4755 - val_accuracy: 0.8575\n",
      "Epoch 14/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.3540 - accuracy: 0.8936 - val_loss: 0.4666 - val_accuracy: 0.8618\n",
      "Epoch 15/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.3416 - accuracy: 0.8973 - val_loss: 0.4589 - val_accuracy: 0.8641\n",
      "Epoch 16/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.3296 - accuracy: 0.9006 - val_loss: 0.4576 - val_accuracy: 0.8654\n",
      "Epoch 17/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.3181 - accuracy: 0.9043 - val_loss: 0.4522 - val_accuracy: 0.8668\n",
      "Epoch 18/100\n",
      "125/125 [==============================] - 28s 227ms/step - loss: 0.3076 - accuracy: 0.9073 - val_loss: 0.4458 - val_accuracy: 0.8687\n",
      "Epoch 19/100\n",
      "125/125 [==============================] - 30s 242ms/step - loss: 0.2972 - accuracy: 0.9102 - val_loss: 0.4449 - val_accuracy: 0.8703\n",
      "Epoch 20/100\n",
      "125/125 [==============================] - 30s 238ms/step - loss: 0.2873 - accuracy: 0.9129 - val_loss: 0.4466 - val_accuracy: 0.8702\n",
      "Epoch 21/100\n",
      "125/125 [==============================] - 28s 220ms/step - loss: 0.2785 - accuracy: 0.9158 - val_loss: 0.4472 - val_accuracy: 0.8716\n",
      "Epoch 22/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.2694 - accuracy: 0.9183 - val_loss: 0.4420 - val_accuracy: 0.8723\n",
      "Epoch 23/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.2614 - accuracy: 0.9206 - val_loss: 0.4434 - val_accuracy: 0.8724\n",
      "Epoch 24/100\n",
      "125/125 [==============================] - 26s 212ms/step - loss: 0.2529 - accuracy: 0.9235 - val_loss: 0.4474 - val_accuracy: 0.8718\n",
      "Epoch 25/100\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 0.2455 - accuracy: 0.9253 - val_loss: 0.4458 - val_accuracy: 0.8731\n",
      "Epoch 26/100\n",
      "125/125 [==============================] - 27s 214ms/step - loss: 0.2378 - accuracy: 0.9276 - val_loss: 0.4484 - val_accuracy: 0.8733\n",
      "Epoch 27/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.2309 - accuracy: 0.9297 - val_loss: 0.4495 - val_accuracy: 0.8726\n",
      "Epoch 28/100\n",
      "125/125 [==============================] - 27s 215ms/step - loss: 0.2238 - accuracy: 0.9317 - val_loss: 0.4457 - val_accuracy: 0.8748\n",
      "Epoch 29/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.2172 - accuracy: 0.9336 - val_loss: 0.4510 - val_accuracy: 0.8748\n",
      "Epoch 30/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.2109 - accuracy: 0.9357 - val_loss: 0.4573 - val_accuracy: 0.8740\n",
      "Epoch 31/100\n",
      "125/125 [==============================] - 28s 221ms/step - loss: 0.2054 - accuracy: 0.9371 - val_loss: 0.4631 - val_accuracy: 0.8733\n",
      "Epoch 32/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.1990 - accuracy: 0.9389 - val_loss: 0.4618 - val_accuracy: 0.8751\n",
      "Epoch 33/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1937 - accuracy: 0.9410 - val_loss: 0.4624 - val_accuracy: 0.8751\n",
      "Epoch 34/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.1883 - accuracy: 0.9422 - val_loss: 0.4611 - val_accuracy: 0.8767\n",
      "Epoch 35/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1833 - accuracy: 0.9438 - val_loss: 0.4717 - val_accuracy: 0.8748\n",
      "Epoch 36/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1786 - accuracy: 0.9453 - val_loss: 0.4685 - val_accuracy: 0.8762\n",
      "Epoch 37/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.1739 - accuracy: 0.9466 - val_loss: 0.4760 - val_accuracy: 0.8752\n",
      "Epoch 38/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1693 - accuracy: 0.9479 - val_loss: 0.4801 - val_accuracy: 0.8750\n",
      "Epoch 39/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.1652 - accuracy: 0.9489 - val_loss: 0.4858 - val_accuracy: 0.8745\n",
      "Epoch 40/100\n",
      "125/125 [==============================] - 27s 217ms/step - loss: 0.1608 - accuracy: 0.9505 - val_loss: 0.4892 - val_accuracy: 0.8748\n",
      "Epoch 41/100\n",
      "125/125 [==============================] - 27s 213ms/step - loss: 0.1566 - accuracy: 0.9519 - val_loss: 0.4909 - val_accuracy: 0.8742\n",
      "Epoch 42/100\n",
      "125/125 [==============================] - 27s 218ms/step - loss: 0.1532 - accuracy: 0.9526 - val_loss: 0.4977 - val_accuracy: 0.8743\n",
      "Epoch 43/100\n",
      "125/125 [==============================] - 27s 212ms/step - loss: 0.1495 - accuracy: 0.9537 - val_loss: 0.4998 - val_accuracy: 0.8746\n",
      "Epoch 44/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.1462 - accuracy: 0.9546 - val_loss: 0.5025 - val_accuracy: 0.8755\n",
      "Epoch 45/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.1426 - accuracy: 0.9559 - val_loss: 0.5046 - val_accuracy: 0.8748\n",
      "Epoch 46/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.1391 - accuracy: 0.9568 - val_loss: 0.5113 - val_accuracy: 0.8748\n",
      "Epoch 47/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.1361 - accuracy: 0.9578 - val_loss: 0.5163 - val_accuracy: 0.8745\n",
      "Epoch 48/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.1337 - accuracy: 0.9584 - val_loss: 0.5206 - val_accuracy: 0.8746\n",
      "Epoch 49/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.1305 - accuracy: 0.9592 - val_loss: 0.5218 - val_accuracy: 0.8749\n",
      "Epoch 50/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.1277 - accuracy: 0.9601 - val_loss: 0.5264 - val_accuracy: 0.8744\n",
      "Epoch 51/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.1251 - accuracy: 0.9610 - val_loss: 0.5325 - val_accuracy: 0.8746\n",
      "Epoch 52/100\n",
      "125/125 [==============================] - 26s 211ms/step - loss: 0.1223 - accuracy: 0.9617 - val_loss: 0.5346 - val_accuracy: 0.8749\n",
      "Epoch 53/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.1200 - accuracy: 0.9625 - val_loss: 0.5354 - val_accuracy: 0.8751\n",
      "Epoch 54/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.1176 - accuracy: 0.9632 - val_loss: 0.5421 - val_accuracy: 0.8749\n",
      "Epoch 55/100\n",
      "125/125 [==============================] - 26s 209ms/step - loss: 0.1154 - accuracy: 0.9634 - val_loss: 0.5465 - val_accuracy: 0.8748\n",
      "Epoch 56/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.1130 - accuracy: 0.9645 - val_loss: 0.5525 - val_accuracy: 0.8740\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 57/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1108 - accuracy: 0.9653 - val_loss: 0.5526 - val_accuracy: 0.8750\n",
      "Epoch 58/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.1088 - accuracy: 0.9657 - val_loss: 0.5622 - val_accuracy: 0.8747\n",
      "Epoch 59/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.1071 - accuracy: 0.9661 - val_loss: 0.5608 - val_accuracy: 0.8740\n",
      "Epoch 60/100\n",
      "125/125 [==============================] - 25s 203ms/step - loss: 0.1047 - accuracy: 0.9669 - val_loss: 0.5662 - val_accuracy: 0.8746\n",
      "Epoch 61/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.1032 - accuracy: 0.9675 - val_loss: 0.5690 - val_accuracy: 0.8748\n",
      "Epoch 62/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.1010 - accuracy: 0.9681 - val_loss: 0.5688 - val_accuracy: 0.8756\n",
      "Epoch 63/100\n",
      "125/125 [==============================] - 25s 202ms/step - loss: 0.0991 - accuracy: 0.9685 - val_loss: 0.5722 - val_accuracy: 0.8749\n",
      "Epoch 64/100\n",
      "125/125 [==============================] - 28s 226ms/step - loss: 0.0973 - accuracy: 0.9691 - val_loss: 0.5832 - val_accuracy: 0.8742\n",
      "Epoch 65/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.0955 - accuracy: 0.9695 - val_loss: 0.5826 - val_accuracy: 0.8743\n",
      "Epoch 66/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0939 - accuracy: 0.9700 - val_loss: 0.5924 - val_accuracy: 0.8733\n",
      "Epoch 67/100\n",
      "125/125 [==============================] - 27s 216ms/step - loss: 0.0926 - accuracy: 0.9704 - val_loss: 0.5918 - val_accuracy: 0.8741\n",
      "Epoch 68/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0908 - accuracy: 0.9710 - val_loss: 0.5970 - val_accuracy: 0.8736\n",
      "Epoch 69/100\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.0892 - accuracy: 0.9713 - val_loss: 0.5971 - val_accuracy: 0.8745\n",
      "Epoch 70/100\n",
      "125/125 [==============================] - 26s 208ms/step - loss: 0.0880 - accuracy: 0.9715 - val_loss: 0.6015 - val_accuracy: 0.8741\n",
      "Epoch 71/100\n",
      "125/125 [==============================] - 27s 217ms/step - loss: 0.0863 - accuracy: 0.9722 - val_loss: 0.6066 - val_accuracy: 0.8746\n",
      "Epoch 72/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.0847 - accuracy: 0.9727 - val_loss: 0.6089 - val_accuracy: 0.8749\n",
      "Epoch 73/100\n",
      "125/125 [==============================] - 28s 222ms/step - loss: 0.0831 - accuracy: 0.9728 - val_loss: 0.6122 - val_accuracy: 0.8739\n",
      "Epoch 74/100\n",
      "125/125 [==============================] - 27s 219ms/step - loss: 0.0824 - accuracy: 0.9732 - val_loss: 0.6140 - val_accuracy: 0.8746\n",
      "Epoch 75/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0807 - accuracy: 0.9736 - val_loss: 0.6176 - val_accuracy: 0.8745\n",
      "Epoch 76/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.0794 - accuracy: 0.9742 - val_loss: 0.6213 - val_accuracy: 0.8745\n",
      "Epoch 77/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0784 - accuracy: 0.9745 - val_loss: 0.6271 - val_accuracy: 0.8738\n",
      "Epoch 78/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0768 - accuracy: 0.9749 - val_loss: 0.6335 - val_accuracy: 0.8730\n",
      "Epoch 79/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.0760 - accuracy: 0.9750 - val_loss: 0.6294 - val_accuracy: 0.8737\n",
      "Epoch 80/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0745 - accuracy: 0.9754 - val_loss: 0.6399 - val_accuracy: 0.8739\n",
      "Epoch 81/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0734 - accuracy: 0.9757 - val_loss: 0.6407 - val_accuracy: 0.8739\n",
      "Epoch 82/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0724 - accuracy: 0.9761 - val_loss: 0.6424 - val_accuracy: 0.8738\n",
      "Epoch 83/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0713 - accuracy: 0.9764 - val_loss: 0.6487 - val_accuracy: 0.8732\n",
      "Epoch 84/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0701 - accuracy: 0.9767 - val_loss: 0.6553 - val_accuracy: 0.8729\n",
      "Epoch 85/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0692 - accuracy: 0.9766 - val_loss: 0.6532 - val_accuracy: 0.8735\n",
      "Epoch 86/100\n",
      "125/125 [==============================] - 26s 210ms/step - loss: 0.0681 - accuracy: 0.9772 - val_loss: 0.6573 - val_accuracy: 0.8733\n",
      "Epoch 87/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0672 - accuracy: 0.9776 - val_loss: 0.6605 - val_accuracy: 0.8726\n",
      "Epoch 88/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0657 - accuracy: 0.9781 - val_loss: 0.6698 - val_accuracy: 0.8732\n",
      "Epoch 89/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0651 - accuracy: 0.9779 - val_loss: 0.6635 - val_accuracy: 0.8739\n",
      "Epoch 90/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.0639 - accuracy: 0.9786 - val_loss: 0.6671 - val_accuracy: 0.8736\n",
      "Epoch 91/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0631 - accuracy: 0.9787 - val_loss: 0.6737 - val_accuracy: 0.8730\n",
      "Epoch 92/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0621 - accuracy: 0.9791 - val_loss: 0.6737 - val_accuracy: 0.8731\n",
      "Epoch 93/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0616 - accuracy: 0.9793 - val_loss: 0.6822 - val_accuracy: 0.8721\n",
      "Epoch 94/100\n",
      "125/125 [==============================] - 26s 205ms/step - loss: 0.0605 - accuracy: 0.9793 - val_loss: 0.6894 - val_accuracy: 0.8726\n",
      "Epoch 95/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0595 - accuracy: 0.9797 - val_loss: 0.6878 - val_accuracy: 0.8727\n",
      "Epoch 96/100\n",
      "125/125 [==============================] - 26s 204ms/step - loss: 0.0589 - accuracy: 0.9800 - val_loss: 0.6886 - val_accuracy: 0.8727\n",
      "Epoch 97/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0578 - accuracy: 0.9801 - val_loss: 0.6924 - val_accuracy: 0.8733\n",
      "Epoch 98/100\n",
      "125/125 [==============================] - 26s 207ms/step - loss: 0.0570 - accuracy: 0.9804 - val_loss: 0.7013 - val_accuracy: 0.8726\n",
      "Epoch 99/100\n",
      "125/125 [==============================] - 25s 204ms/step - loss: 0.0566 - accuracy: 0.9806 - val_loss: 0.6980 - val_accuracy: 0.8724\n",
      "Epoch 100/100\n",
      "125/125 [==============================] - 26s 206ms/step - loss: 0.0556 - accuracy: 0.9808 - val_loss: 0.7034 - val_accuracy: 0.8725\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1dc255c5400>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data, batch_size = batch_size, epochs=epochs, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### saving our model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('saved models/English-French Translation.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL = load_model('saved models/English-French Translation.h5')\n",
    "MODEL.compile(optimizer = 'rmsprop', loss='categorical_crossentropy', metrics =['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tensorflow.python.keras.engine.input_layer.InputLayer at 0x24a619d74f0>,\n",
       " <tensorflow.python.keras.engine.input_layer.InputLayer at 0x24a619d7490>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x24a619d71f0>,\n",
       " <tensorflow.python.keras.layers.recurrent_v2.LSTM at 0x24a619c7d30>,\n",
       " <tensorflow.python.keras.layers.core.Dense at 0x24a4eab18b0>]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MODEL.layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Execution TIME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Encoder layer\n",
    "encoder = MODEL.layers[2]\n",
    "encoder_inputs = MODEL.layers[0].input\n",
    "encoder_outputs, state_h, state_c = MODEL.layers[2].output\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "encoder_model = Model(inputs= encoder_inputs, outputs= encoder_states)\n",
    "#it takes numpy array input and returns output in the form of list of 2 elements."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Encoder Model For Inference:\n",
    "https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Graph-of-Encoder-Model-For-Inference.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, None, 71)]        0         \n",
      "_________________________________________________________________\n",
      "lstm (LSTM)                  [(None, 256), (None, 256) 335872    \n",
      "=================================================================\n",
      "Total params: 335,872\n",
      "Trainable params: 335,872\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "encoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining Decoder layer\n",
    "\n",
    "decoder_inputs = MODEL.layers[1].input\n",
    "decoder_lstm = MODEL.layers[3]\n",
    "decoder_dense = MODEL.layers[-1]\n",
    "\n",
    "decoder_state_input_h = Input(shape= (latent_dim,))\n",
    "decoder_state_input_c = Input(shape= (latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs,\n",
    "                                                 initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs=decoder_dense(decoder_outputs)\n",
    "\n",
    "decoder_model = Model(inputs = [decoder_inputs] + decoder_states_inputs,\n",
    "                     outputs = [decoder_outputs] + decoder_states)\n",
    "# it takes and returns list of 3 elements.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Graph of Decoder Model For Inference:\n",
    "https://3qeqpr26caki16dnhd19sv6by6v-wpengine.netdna-ssl.com/wp-content/uploads/2017/10/Graph-of-Decoder-Model-For-Inference.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            [(None, None, 92)]   0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_5 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_6 (InputLayer)            [(None, 256)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                   [(None, None, 256),  357376      input_2[0][0]                    \n",
      "                                                                 input_5[0][0]                    \n",
      "                                                                 input_6[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, None, 92)     23644       lstm_1[2][0]                     \n",
      "==================================================================================================\n",
      "Total params: 381,020\n",
      "Trainable params: 381,020\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "decoder_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "#reverse-lookup token index to decode sequences back to something readable.\n",
    "reverse_input_char_index = dict((i,char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i,char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    \n",
    "    states_value = encoder_model.predict(input_seq)# this model outputs only list of encoder states..\n",
    "    \n",
    "    target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "    \n",
    "    target_seq[0,0, target_token_index['\\t']] = 1.\n",
    "    \n",
    "  \n",
    "    stop_condition=False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "       \n",
    "        sampled_token_index = np.argmax(output_tokens[0,-1,:])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        \n",
    "        \n",
    "        if (sampled_char == '\\n' or len(decoded_sentence) > max_decoder_seq_len):\n",
    "            stop_condition = True\n",
    "            \n",
    "        #update the target sequence (of length 1)\n",
    "        target_seq = np.zeros((1,1, num_decoder_tokens))\n",
    "        target_seq[0,0, sampled_token_index] =1.\n",
    "        \n",
    "        #update states\n",
    "        states_value = [h,c]\n",
    "        \n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------\n",
      "Input sentence:  Go.\n",
      "Decoded sentence:  Va !\n",
      "\n",
      "---------------------------\n",
      "Input sentence:  Hi.\n",
      "Decoded sentence:  Salut.\n",
      "\n",
      "---------------------------\n",
      "Input sentence:  Hi.\n",
      "Decoded sentence:  Salut.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(3):\n",
    "    #take the sequence (part of the training set) for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index +1] #since encoder takes 3-D input..\n",
    "    #above line can also be replaced as input_seq = encoder_input_data[seq_index].reshape((1,) + encoder_input_data[seq_index])\n",
    "    \n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('---------------------------')\n",
    "    print('Input sentence: ', input_texts[seq_index])\n",
    "    print('Decoded sentence: ', decoded_sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
